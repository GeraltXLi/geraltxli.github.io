<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://geraltxli.github.io</id>
    <title>GeraltXLi</title>
    <updated>2021-11-19T14:47:33.207Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://geraltxli.github.io"/>
    <link rel="self" href="https://geraltxli.github.io/atom.xml"/>
    <subtitle>The flash that cuts through darkness, the light that breaks the night.&lt;br&gt;&lt;br&gt;
    Total &lt;span id=&quot;busuanzi_value_site_uv&quot;&gt;&lt;/span&gt; visitors.
&lt;/span&gt;</subtitle>
    <logo>https://geraltxli.github.io/images/avatar.png</logo>
    <icon>https://geraltxli.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, GeraltXLi</rights>
    <entry>
        <title type="html"><![CDATA[记一次服务端OOM问题排查过程与解决方法]]></title>
        <id>https://geraltxli.github.io/post/ji-yi-ci-fu-wu-duan-oom-wen-ti-pai-cha/</id>
        <link href="https://geraltxli.github.io/post/ji-yi-ci-fu-wu-duan-oom-wen-ti-pai-cha/">
        </link>
        <updated>2021-11-17T15:04:05.000Z</updated>
        <summary type="html"><![CDATA[<p>记灰度环境一次碰到服务OOM的问题的排查过程与解决方法</p>
]]></summary>
        <content type="html"><![CDATA[<p>记灰度环境一次碰到服务OOM的问题的排查过程与解决方法</p>
<!-- more -->
<h1 id="记一次服务端oom问题排查过程与解决方法">记一次服务端OOM问题排查过程与解决方法</h1>
<h2 id="背景">背景</h2>
<p>起因是改了服务算法里匹配部分的代码，从串行改为并行，然后丢到灰度环境，重放线上流量时，容器内存占用上涨不少，几G有时候会上涨到二十G，然后触发OOMKILL。</p>
<h2 id="排查过程">排查过程</h2>
<h3 id="profile直接看实际内存占用">profile直接看实际内存占用</h3>
<blockquote>
<p>首先排除是Golang runtime归还内存给操作系统的策略导致，因为之前踩过MADV_FREE的坑。现在用的版本是Go1.16，归还策略已经改回去了</p>
</blockquote>
<ol>
<li>查看内存占用，采集上涨前后对比，发现inuse的堆内存并没有实际那么高，最多才1G，而且前后没有明显的上涨，跟监控里服务实际使用内存相差太多了</li>
<li>进一步看meminfo分析，服务在不断进行大量分配和回收内存</li>
<li>切换一下，查看alloc的内存，发现大量分配内存在代码里两个字符串最长公共子序列LCS的计算上</li>
<li>观察算法，就是很正常M*N的二维数组动规算法来算LCS，咋一看好像没啥问题，总共才十来行代码</li>
<li>拿线上数据本地跑一下发现，有几千词的字符串占用大小是30KB左右，跟相同差不多长度的匹配时，一个gorouine就上百MB，我改成并行后就上1GB往上了，更别说实际服务有几十个worker运行，所以的确能触发这么高的内存占用</li>
</ol>
<h2 id="处理方法">处理方法</h2>
<p>两个方向：</p>
<ol>
<li>其实来到算法这里的字符串正常不应该这么长，在前面步骤应该裁剪到一定程度，不过这是业务部分的逻辑。</li>
<li>优化DP算法，正常DP的状态转移记录都是可以压缩的，把二维数组压缩到一维，将O(N*M)的空间复杂度减少至O(N)复杂度，即可解决。</li>
</ol>
<h2 id="后记">后记</h2>
<p>改完后内存占用直接降到1G以内，数据结构与算法果然是第一生产力。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[iOS稳定性问题治理-笔记]]></title>
        <id>https://geraltxli.github.io/post/ios-wen-ding-xing-wen-ti-zhi-li-bi-ji/</id>
        <link href="https://geraltxli.github.io/post/ios-wen-ding-xing-wen-ti-zhi-li-bi-ji/">
        </link>
        <updated>2021-11-14T13:36:56.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<p>最近参加ArchSummit，听字节APM线分享iOS的OOM和Watchdog等问题的治理，刚好工作也相关，收获不少，做个笔记</p>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<p>最近参加ArchSummit，听字节APM线分享iOS的OOM和Watchdog等问题的治理，刚好工作也相关，收获不少，做个笔记</p>
</blockquote>
<!-- more -->
<h2 id="分类">分类</h2>
<ul>
<li>OOM</li>
<li>Watchdog</li>
<li>Crash</li>
<li>Disk I/O 异常</li>
<li>CPU异常</li>
</ul>
<h2 id="oom">OOM</h2>
<p>OOM问题没有明确调用栈，也很难复现，字节解决方法是MemoryGraph</p>
<ol>
<li>App物理内存占用超过一定值（根据实际情况设定）就会内存dump下来，</li>
<li>对各个内存节点进行符号化（SDK直接做符号化的话，系统库这部分没问题，不过APP程序符号不是strip掉了么，不过实际操作都好解决）</li>
<li>和记录彼此的强弱引用关系（通过内存地址来判断相互之间的引用关系）</li>
<li>上报到后台分析大内存占用和内存泄露问题。</li>
</ol>
<p>常见原因：</p>
<ul>
<li>内存泄露</li>
<li>内存堆积</li>
<li>资源异常</li>
<li>内存使用不当</li>
</ul>
<h2 id="watchdog">Watchdog</h2>
<p>方法;</p>
<ol>
<li>
<p>多次抓栈并记录主线程的线程状态（其他线程也会抓，但相对主线程来说没那么频繁）</p>
</li>
<li>
<p>根据调用栈，CPU占用和状态，线程标记来判断是否存在死锁或者死循环</p>
</li>
</ol>
<p>一般排查卡顿的确也会考虑定期从寄存器抓取线程堆栈，维护一定时间区间内的栈信息（毕竟卡顿时刻的堆栈并不一定是原因所在，所以需要是区间），再检测到卡顿时上报到后端，辅助排查。不过就是要注意性能开销，，免得检测工具本身反而成为影响性能的原因。</p>
<p>常见原因:</p>
<ul>
<li>
<p>死锁</p>
</li>
<li>
<p>锁竞争</p>
</li>
<li>
<p>主线程IO</p>
</li>
<li>
<p>跨进程通信</p>
</li>
</ul>
<h2 id="crash">Crash</h2>
<p>方法：</p>
<ul>
<li>生成coredump（常见操作，没啥好说的，有当前时刻的完整运行状态）</li>
<li>iOS线上开启<code>Zombie Object</code>，就是OC对象销毁hook掉，生成僵尸对象，只要野指针指向僵尸对象，再次访问就会崩溃在那里，这样就能拿到真正的错误现场（值得注意的是，这里相当于内存没有回收掉，某种意义上的内存泄露，所以线上开启时必须要对对象的数量有限制）</li>
</ul>
<h2 id="cpudisk-io">CPU&amp;Disk I/O</h2>
<p>基于MetricKit框架获取处理（我只是一个小服务端，不大了解苹果这框架，就酱）</p>
<h2 id="参考内容">参考内容</h2>
<ul>
<li>字节ArchSummit上分享的PPT  <a href="https://ppt.infoq.cn/slide/download?cid=85&amp;pid=3581">如何系统性治理iOS稳定性问题-丰亚东</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golang脑图]]></title>
        <id>https://geraltxli.github.io/post/golang-nao-tu/</id>
        <link href="https://geraltxli.github.io/post/golang-nao-tu/">
        </link>
        <updated>2021-08-15T16:02:32.000Z</updated>
        <summary type="html"><![CDATA[<p>Golang知识图谱，梳理知识</p>
]]></summary>
        <content type="html"><![CDATA[<p>Golang知识图谱，梳理知识</p>
<!-- more -->
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clickhosue 底层存储结构（学习笔记）]]></title>
        <id>https://geraltxli.github.io/post/clickhosue-di-ceng-cun-chu-jie-gou-xue-xi-bi-ji/</id>
        <link href="https://geraltxli.github.io/post/clickhosue-di-ceng-cun-chu-jie-gou-xue-xi-bi-ji/">
        </link>
        <updated>2021-07-31T09:56:55.000Z</updated>
        <summary type="html"><![CDATA[<p>上一篇<a href="https://geraltxli.github.io/post/clickhouse-xiang-jie/">Clickhouse介绍</a>大概描述了clickhouse的特性，这里讲讲实际底层的存储结构</p>
]]></summary>
        <content type="html"><![CDATA[<p>上一篇<a href="https://geraltxli.github.io/post/clickhouse-xiang-jie/">Clickhouse介绍</a>大概描述了clickhouse的特性，这里讲讲实际底层的存储结构</p>
<!-- more -->
<h2 id="mergetree存储结构">MergeTree存储结构</h2>
<p>查看clickhouse磁盘上的表数据结构，基本上就是下方的目录结构。表名目录下有很多分区，除非表没有设置分区，完全就只在all分区上。</p>
<pre><code class="language-bash">└─clickhouse
    └─table
        └─partition_1
            └─checksum.txt(当前目录各个文件的大小以及各文件内容的hash)
            └─columns.txt（设计所有列和列的类型）
            └─count.txt（此part数据行数）
            └─primary.idx （主键稀疏索引，找到bin里面对应的block）
            └─[Column].bin （对应列的实际数据内容）
            └─[Column].mrk （对应列的标记文件，辅助索引进一步找到block内的granularity）
用了分区键   └─partition.dat (用了分区间)
用了分区键   └─minmax_[Column].idx
用二级索引   └─skp_idx_[Column].idx
用二级索引   └─skp_idx_[Column].mrk
</code></pre>
<p>一次数据写入请求</p>
<ul>
<li>数据根据<code>index_granularity</code>（默认8192）切成多个颗粒（granularity）</li>
<li>多个<code>granularity</code>在内存中积攒一定大小，按照其压缩前的数据字节大小，都被严格的控制在<strong>64K~1M</strong>之间，其上下限分别由<code>min_compress_block_size</code>(默认65536)与<code>max_compress_block_size</code>(默认1048576)参数指定。满足条件后触发压缩和落盘，形成一个block（clickhouse与磁盘交互扫描的最小单位）
<ul>
<li>头文件（压缩方法，压缩前后大小）</li>
<li>压缩数据</li>
</ul>
</li>
<li>多个block落地成一个bin文件（数据内容），和其他相关文件一起落到同一个part。即<strong>新增了一个目录</strong>，所以官方才会建议insert频率降低或者一次insert的batch size要足够大，否则会影响IO和目录数过大耗尽inode，影响数据合并性能</li>
</ul>
<h2 id="参考">参考</h2>
<ul>
<li>
<p><a href="https://www.cnblogs.com/eedbaa/p/14512803.html">Clickhouse数据存储结构</a></p>
</li>
<li>
<p><a href="https://blog.csdn.net/weixin_34148340/article/details/94320571">clickhouse底层存储原理</a></p>
</li>
<li>
<p><a href="https://cloud.tencent.com/developer/article/1814617">ClickHouse高性能列存核心原理</a></p>
</li>
<li>
<p><a href="https://github.com/ClickHouse/clickhouse-presentations/blob/master/meetup32/%E6%9C%B1%E5%87%AF.ppt">Clickhouse MergeTree原理解析</a></p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clickhouse 介绍]]></title>
        <id>https://geraltxli.github.io/post/clickhouse-xiang-jie/</id>
        <link href="https://geraltxli.github.io/post/clickhouse-xiang-jie/">
        </link>
        <updated>2021-07-25T07:02:50.000Z</updated>
        <summary type="html"><![CDATA[<p>概述OLAP场景与列式数据库特征，以及Clickhouse这数据库的各个特性（各个特性的组合实现了clickhouse的高性能）</p>
]]></summary>
        <content type="html"><![CDATA[<p>概述OLAP场景与列式数据库特征，以及Clickhouse这数据库的各个特性（各个特性的组合实现了clickhouse的高性能）</p>
<!-- more -->
<h1 id="clickhouse-详解">Clickhouse 详解</h1>
<h2 id="一-概述">一、概述</h2>
<blockquote>
<p>ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).</p>
</blockquote>
<p>根据官方文档说法，它是列式数据库，应用于OLAP场景。</p>
<h3 id="olap">OLAP</h3>
<p>OLAP(OnLine Analysis Processing ，联机分析处理 )，是数据仓库系统的主要应用，从多维度，多层次的角度，针对特定问题进行数据的汇总分析。</p>
<h4 id="特征">特征</h4>
<blockquote>
<p>OLAP场景的关键特征<a href="https://clickhouse.tech/docs/zh/#olapchang-jing-de-guan-jian-te-zheng">¶</a></p>
<ul>
<li>绝大多数是读请求</li>
<li>数据以相当大的批次(&gt; 1000行)更新，而不是单行更新;或者根本没有更新。</li>
<li>已添加到数据库的数据不能修改。</li>
<li>对于读取，从数据库中提取相当多的行，但只提取列的一小部分。</li>
<li>宽表，即每个表包含着大量的列</li>
<li>查询相对较少(通常每台服务器每秒查询数百次或更少)</li>
<li>对于简单查询，允许延迟大约50毫秒</li>
<li>列中的数据相对较小：数字和短字符串(例如，每个URL 60个字节)</li>
<li>处理单个查询时需要高吞吐量(每台服务器每秒可达数十亿行)</li>
<li>事务不是必须的</li>
<li>对数据一致性要求低</li>
<li>每个查询有一个大表。除了他以外，其他的都很小。</li>
<li>查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的RAM中</li>
</ul>
</blockquote>
<h3 id="列式数据库">列式数据库</h3>
<p>传统关系数据库的数据一般在按行存储，即一行数据在物理存储上位置是连续的，而列式数据库则是按列存储。值得注意的是，这里列式或者行式只是指数据库的存储模型（storage model），并不影响上层的数据模型（data model)，列式存储的数据库可能是SQL数据库或者NOSQL数据库。</p>
<h4 id="特征与优点">特征与优点：</h4>
<ul>
<li>数据库按列存储</li>
<li>数据即是索引</li>
<li>只访问查询涉及的列 - 大量降低系统IO</li>
<li>每列可单独线程处理 - 通过并发提高查询速度</li>
<li>数据类型一致，数据特征相似 - 高效压缩</li>
</ul>
<h4 id="优点">优点：</h4>
<ul>
<li><strong>更快的IO</strong>：对指定列查询统计分析时，因为只涉及指定列，不像行式会查询到其他列数据，减少IO，并且一定程度上随机读取变成顺序读取</li>
<li><strong>更低的存储（成本更低）</strong>：同一列数据更好压缩（比如字典表压缩）</li>
<li><strong>更低的内存和CPU</strong>：查询涉及的数据量低时（无论是因为只读了相关列或者是列的压缩），占用计算内存更少。</li>
</ul>
<h4 id="缺点">缺点：</h4>
<ul>
<li><strong>组装消耗</strong>：select完成后需要组装各个列数据</li>
<li><strong>插入更新更麻烦</strong>：比如一次插入，需要对多列操作，比如不同列在不同page，需要进行多次IO（行式可能只需一次）</li>
</ul>
<hr>
<h4 id="clickhouse-features官方文档说法">Clickhouse Features（官方文档说法）</h4>
<ul>
<li>
<p><strong>列式存储</strong>：In a real column-oriented DBMS, no extra data is stored with the values.  即数据不会有额外数据（比如记录数据长度）</p>
</li>
<li>
<p><strong>数据压缩</strong>：除了高效通用压缩codec（编解码器）之外，clickhouse还对特定类型的数据提供专门的codec</p>
</li>
<li>
<p><strong>磁盘存储</strong>：因为数据在物理存储上就是按主键排序了的，可以以低延迟（不到几十毫秒）提取特定值或者范围数据。Clickhouse设计是用于常规硬盘，成本更低，但如果允许，SSD或者更多RAM可以进一步提高性能</p>
</li>
<li>
<p><strong>多核并行处理</strong>：Clickhouse实际应用是可以跑满CPU内存资源的。</p>
</li>
<li>
<p><strong>多服务器上的分布式查询</strong>：数据多分片存储，各分片也可以有多个副本</p>
</li>
<li>
<p><strong>SQL支持</strong>：大部分情况满足ANSI SQL标准</p>
</li>
<li>
<p><strong>向量计算引擎</strong>（可见参考描述）：</p>
<ul>
<li>SIMD即<strong>single instruction, multiple data&quot;（单指令流多数据流)</strong>，本质上是采用一个控制器来控制多个处理器，同时对一组数据中的每一条分别执行相同的操作，从而实现空间上的并行性的技术。</li>
<li>CPU通过SSE指令集实现SIMD</li>
<li>clickhouse充分应用SSE指令集：filter（批量处理128位）或者大小写转换（一次加载16个字段转换）</li>
</ul>
</li>
<li>
<p><strong>实时数据更新</strong>：支持主键表，为了快速执行基于主键的范围查询，Clickhouse会使用合并树对数据进行增量排序，数据可以实时不断添加。（不过要特别注意clickhouse对于各个数据part的合并，合并的时机不定，实际业务也要有意识控制part大小）</p>
</li>
<li>
<p><strong>主键索引</strong>：查询主键的特定值或者特定范围的延迟非常低（不到几十毫秒），顺序读写</p>
</li>
<li>
<p><strong>二级索引</strong>：与其他数据库不同，clickhouse的二级索引不指向特定行或者行范围。相反，它们让数据库提前知道某些数据部分中的所有行都不会匹配查询过滤条件并且根本不读取它们，因此它们被称为<a href="https://clickhouse.tech/docs/en/engines/table-engines/mergetree-family/mergetree/#table_engine-mergetree-data_skipping-indexes"> data skipping indexes.</a>。比如：记录这个data part的年龄最小值是30岁，那么查询30岁以下数据时，就直接跳过整个data part，加快查询。</p>
</li>
<li>
<p><strong>支持在线查询</strong>：简而言之，就是查询太快了，所以允许在线实时查询，不需要像其他OLAP场景下跑离线任务。</p>
</li>
<li>
<p><strong>支持近似值的计算</strong>：ClickHouse提供各种各样在允许牺牲数据精度的情况下对查询进行加速的方法：</p>
<ul>
<li>
<p>用于近似计算的各类聚合函数，如：distinct values, medians, quantiles</p>
</li>
<li>
<p>基于数据的部分样本进行近似查询。这时，仅会从磁盘检索少部分比例的数据。</p>
</li>
<li>
<p>不使用全部的聚合条件，通过随机选择有限个数据聚合条件进行聚合。这在数据聚合条件满足某些分布条件下，在提供相当准确的聚合结果的同时降低了计算资源的使用。</p>
</li>
</ul>
</li>
<li>
<p><strong>自适应的Join算法</strong></p>
</li>
<li>
<p><strong>数据复制和完整性支持</strong>：ClickHouse使用异步的多主复制技术。当数据被写入任何一个可用副本后，系统会在后台将数据分发给其他副本，以保证系统在不同副本上保持相同的数据。在大多数情况下ClickHouse能在故障后自动恢复，在一些少数的复杂情况下需要手动恢复。</p>
</li>
<li>
<p><strong>基于角色的访问控制</strong></p>
</li>
<li>
<p><strong>可视为缺点的feature</strong></p>
<ul>
<li>没有完整的事务支持。</li>
<li>缺少高频率，低延迟的修改或删除已存在数据的能力。仅能用于批量删除或修改数据，但这符合 <a href="https://gdpr-info.eu/">GDPR</a>。</li>
<li>稀疏索引使得ClickHouse不适合通过其键检索单行的点查询。</li>
</ul>
</li>
</ul>
<h2 id="二-性能">二、性能</h2>
<h4 id="描述">描述</h4>
<p>根据<a href="https://clickhouse.tech/docs/en/introduction/performance/">官方文档</a>说法</p>
<ul>
<li>单个查询的吞吐量，提取一个10字节的列，处理速度大概是1-2亿行每秒</li>
<li>普通短查询正常小于50ms</li>
</ul>
<p>另外值得注意的是</p>
<ul>
<li>写入性能最好是批量写入，每次不少于1000行或者一秒一次插入。</li>
<li>写入的数据每行为1Kb，那么写入的速度为50，000到200，000行每秒（我实际业务测试时，单机十四万行插入为2秒多）</li>
<li>并发查询不要超过100/s</li>
</ul>
<h4 id="为啥这么快">为啥这么快</h4>
<p>看列式数据库的特征和clickhouse本身feature就大概知道了，列式存储加上压缩，减少读取数据，更少的IO，相同查询占用更少内存（成本也更低）。同时支持并发处理，clickhouse本身对于压缩和数据向量并行处理又做了优化，能更好地利用CPU，多核并行处理。</p>
<h2 id="三-备注">三、备注</h2>
<p>以上基本就是自己扫一遍clickhouse官方文档的特性，理解一下列式数据库和clickhouse特性实际上大概做了什么事情。有啥问题欢迎指正~</p>
<h1 id="参考">参考</h1>
<ul>
<li><a href="https://clickhouse.tech/docs/en/">官方文档</a></li>
<li><a href="https://blog.csdn.net/nieson2012/article/details/79551337">什么是列式存储数据库</a></li>
<li><a href="https://www.jianshu.com/p/c36f4e142b8a">漫谈SIMD、SSE指令集与Clickhouse向量化执行</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[服务端开发规范（未完待续）]]></title>
        <id>https://geraltxli.github.io/post/fu-wu-duan-kai-fa-gui-fan-todo/</id>
        <link href="https://geraltxli.github.io/post/fu-wu-duan-kai-fa-gui-fan-todo/">
        </link>
        <updated>2021-07-08T14:53:09.000Z</updated>
        <summary type="html"><![CDATA[<p>服务端开发遵守的Git, CHANGELOG, Version规范。</p>
]]></summary>
        <content type="html"><![CDATA[<p>服务端开发遵守的Git, CHANGELOG, Version规范。</p>
<!-- more -->
<h2 id="开发规范">开发规范</h2>
<ul>
<li><a href="https://www.conventionalcommits.org/en/v1.0.0/">git commit规范</a></li>
<li><a href="https://keepachangelog.com/en/1.0.0/">changelog规范</a></li>
<li><a href="https://semver.org/lang/zh-CN/">版本规范</a></li>
<li><a href="https://github.com/golang-standards/project-layout">Go项目目录结构</a></li>
</ul>
<p>未完待续...</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[微服务治理- 学习笔记（一）]]></title>
        <id>https://geraltxli.github.io/post/wei-fu-wu-zhi-li-fa-zhan-xue-xi/</id>
        <link href="https://geraltxli.github.io/post/wei-fu-wu-zhi-li-fa-zhan-xue-xi/">
        </link>
        <updated>2021-06-19T07:52:42.000Z</updated>
        <summary type="html"><![CDATA[<p>微服务是一种研发模式，不是开发里引入微服务框架即可，而是涉及整个开发周期。<br>
最近在看《微服务治理》，顺便做下笔记，毕竟好记性不如烂键盘？</p>
]]></summary>
        <content type="html"><![CDATA[<p>微服务是一种研发模式，不是开发里引入微服务框架即可，而是涉及整个开发周期。<br>
最近在看《微服务治理》，顺便做下笔记，毕竟好记性不如烂键盘？</p>
<!-- more -->
<h2 id="1-单体架构">1. 单体架构</h2>
<h3 id="架构分层与组件库">架构分层与组件库</h3>
<p>最初在网站或者系统较小的时候，一般都是把所有功能模块打包一在一个工程直接部署。在我学生时期的大作业，或者刚工作时碰到功能较为简单或者访问量较小的网站之类就是如此。</p>
<ul>
<li>设计模式使用<strong>MVC</strong></li>
<li>实际开发通过<strong>框架</strong>(Framework)完成规范的约束</li>
<li>将通用功能模块抽离成功独立的<strong>通用组件库</strong>，解放业务开发人员对底层的关注。不过工程与组件库之间的依赖和组件库之间的依赖会组件形成网状关系，然后版本迭代进一步提升复杂度</li>
</ul>
<h3 id="不足">不足</h3>
<p>不过随着时间的推移，代码的不断迭代，整个工程终究会不断变成一个庞大的怪兽（如果一直活着）</p>
<ul>
<li>单体应用庞大又复杂时，甚至开发人员都发生了变更，没有开发者对其有完全了解，这时候无论是修BUG还是功能迭代，都会变得十分谨慎，容易恶性循环，谁都不愿意优化或者改动过大，单纯堆砌直到重构</li>
<li>单体应用庞大影响启停速度，从而影响开发效率与线上变更</li>
<li>单体应用碰到资源冲突时难以扩展，比如不同模块可能是cpu密集型或者io密集型，或者内存型之类的，部署成本更高</li>
<li>最后就是尾大不掉，太大了，难以进行架构升级与技术优化</li>
</ul>
<hr>
<h2 id="2-soa">2. SOA</h2>
<p>形式是服务总线（ESB）</p>
<ul>
<li>传统中间件技术加通信协议</li>
<li>由老牌大厂主导，复杂，庞大</li>
<li>服务治理粗糙，人工比例大</li>
</ul>
<p>不过我毕业后在互联网行业倒基本没怎么碰到过这个。</p>
<hr>
<h2 id="3-分布式服务与治理">3. 分布式服务与治理</h2>
<h3 id="背景">背景</h3>
<ul>
<li>互联网（xx敏捷开发)，版本迭代非常快</li>
<li>无论是网站还是应用，各个功能模块访问不均衡</li>
</ul>
<p>随着发展规模壮大，按业务拆分服务</p>
<h3 id="服务框架">服务框架</h3>
<p>轻量化SOA架构</p>
<ul>
<li>引入服务注册发现机制与远程调用机制</li>
<li>服务负载均衡</li>
<li>限流，降级和熔断保护机制</li>
</ul>
<p>将服务管控与运维能力下沉到框架层面，让业务开发者专注开发</p>
<h3 id="分布式服务治理">分布式服务治理</h3>
<ul>
<li>服务度量
<ul>
<li>服务基础与管理信息</li>
<li>服务质量和服务能力（支持XXXqps）</li>
<li>服务依赖和服务调用统计</li>
<li>服务物理分布</li>
</ul>
</li>
<li>服务管控
<ul>
<li>服务部署上下线</li>
<li>服务路由管控</li>
<li>服务限流，降级和熔断保护措施</li>
<li>服务授权（访问授权管理）</li>
</ul>
</li>
</ul>
<hr>
<h2 id="4-微服务及治理">4. 微服务及治理</h2>
<blockquote>
<p>微服务是一种研发模式，不止技术</p>
</blockquote>
<h3 id="背景-2">背景</h3>
<ol>
<li>docker技术的出现，极大降级服务部署与运维成本</li>
<li>服务应用规模越来越大，服务粒度越来越小（比如单纯的缓存服务）</li>
</ol>
<h3 id="架构模式特点">架构模式特点</h3>
<ul>
<li><strong>高内聚，低耦合。</strong> 根据业务边界确定服务边界，符合领域驱动设计（DDD），专心完成一件不可再分割的完整业务操作功能，即为微服务</li>
<li><strong>资源独享</strong>：像数据库资源便是独享，确保松耦合。不过会导致数据冗余，事务一致性也很麻烦</li>
<li><strong>模式</strong>：
<ul>
<li><strong>基于SDK的同构</strong>，由框架提供微服务一系列管控能力</li>
<li><strong>ServiceMesh(SideCar)</strong>，可以异构，sidecar就相当于一个透明代理，处理微服务路由与管控事情。</li>
</ul>
</li>
<li><strong>问题</strong>：
<ul>
<li><strong>单点依赖</strong>：存在通用服务被大部分其他服务调用（比如来个鉴权服务跪了，所有依赖的相关服务涉及鉴权的业务都得）</li>
<li><strong>循环调用</strong>：A-&gt;B-&gt;C-&gt;A</li>
<li><strong>服务冗余</strong>： 存在微服务已不再使用</li>
</ul>
</li>
<li><strong>调用关系梳理</strong>：
<ul>
<li>静态代码调用链路分析</li>
<li>动态线上调用依赖关系分析</li>
</ul>
</li>
</ul>
<h3 id="研发治理">研发治理</h3>
<ul>
<li>
<p><strong>小团队，小工程</strong>：不同微服务做好隔离，每个服务都是独立工程，提高各个工程的构建与部署效率；不要一个大工程，一个目录一个模块，CI构建不同微服务。（对于笔者实际工作经验来说，在开发某个应用的时候的确会出现这样的设计，原因应该就是多个工程创建繁琐，同一工程方便引用代码）</p>
</li>
<li>
<p><strong>工程与代码质量治理</strong>：正常都是发生在业务快速迭代期间，功能迭代，时间紧和任务重。要不临时修改，要不严格按照规范执行。（如果没有强制约束，非常容易出现先用临时方案做出来，后续有空再重构！！！实际上只要没出问题，一般不会有空去重构的）</p>
</li>
<li>
<p><strong>接口契约治理</strong>： 以接口形式提供内部功能，服务提供方需要确保变更不影响接口逻辑，或者保证变更能同步到调用方</p>
</li>
<li>
<p><strong>测试治理</strong>：单元测试和冒烟测试需要Mock，但也有局限（工作量，无法模拟网络延时等）；集成测试对线上服务完备性依赖强</p>
</li>
<li>
<p><strong>运维治理</strong>：服务集群节点规模大，调用层级多且复杂。需要对线上有完备的链路追踪与监控，能及时发现节点健康问题，进行修复或者资源调度；智能化运维（像笔者工作中便做过服务端日志智能归类平台，收集服务端日志，过滤清洗，归类统计与报警，减少程序工作量。）</p>
</li>
<li>
<p><strong>管理治理</strong>：敏捷开发（毕竟微服务架构即是灵活快速）；Devops，运维自动化将部署运维监控与管理让渡给开发者。如书中图（微服务全生命周期管理）</p>
<figure data-type="image" tabindex="1"><img src="https://geraltxli.github.io/post-images/1624098421948.png" alt="" loading="lazy"></figure>
<p>以上单纯是看书学习时做的一点笔记，有问题欢迎指正。</p>
<h2 id="参考">参考</h2>
<ul>
<li>《微服务治理》（刚开始看这本书）</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ClickHouse单机安装，高可用集群部署和简单使用]]></title>
        <id>https://geraltxli.github.io/post/clickhouse-dan-ji-an-zhuang-he-gao-ke-yong-ban-bu-shu/</id>
        <link href="https://geraltxli.github.io/post/clickhouse-dan-ji-an-zhuang-he-gao-ke-yong-ban-bu-shu/">
        </link>
        <updated>2021-06-02T15:16:23.000Z</updated>
        <summary type="html"><![CDATA[<p>最近用于OLAP场景的列式数据库ClickHouse挺火的，这里简单讲一下安装，部署和使用。不过阿里云都有SaaS版直接使用了，没有的才要自己搭🤡</p>
]]></summary>
        <content type="html"><![CDATA[<p>最近用于OLAP场景的列式数据库ClickHouse挺火的，这里简单讲一下安装，部署和使用。不过阿里云都有SaaS版直接使用了，没有的才要自己搭🤡</p>
<!-- more -->
<h1 id="单机版">单机版</h1>
<h2 id="1-安装部署">1. 安装部署</h2>
<p><a href="https://clickhouse.tech/docs/en/getting-started/tutorial/">官方文档</a></p>
<p>单机安装，怎么快怎么来，实际上就这些：</p>
<pre><code class="language-bash">sudo apt-get install apt-transport-https ca-certificates dirmngr
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4

echo &quot;deb https://repo.clickhouse.tech/deb/stable/ main/&quot; | sudo tee \
    /etc/apt/sources.list.d/clickhouse.list
sudo apt-get update

sudo apt-get install -y clickhouse-server clickhouse-client

sudo service clickhouse-server start
clickhouse-client
</code></pre>
<h2 id="2-启动命令">2. 启动命令</h2>
<p>如果找不到service的话，debian可以直接去这目录，另外也可以看看实际启动命令是啥，</p>
<p>启动：</p>
<pre><code class="language-bash">$debian /usr/sbin/service clickhouse-server start

# 启动配置见
vim /etc/systemd/system/clickhouse-server.service
</code></pre>
<h2 id="3-配置文件">3. 配置文件</h2>
<h3 id="存储配置">存储配置</h3>
<p>这里是参考官方文档抄的存储策略，直接写在<code>config.d</code>目录下的<code>storage.xml</code><br>
(默认SSD，触发条件时迁移hdd)</p>
<pre><code class="language-xml">&lt;yandex&gt;
    &lt;storage_configuration&gt;
        &lt;disks&gt;
            &lt;fast_ssd&gt;
                &lt;path&gt;/data/fast_ssd/&lt;/path&gt;
            &lt;/fast_ssd&gt;
            &lt;hdd1&gt;
                &lt;path&gt;/home/clickhouse/hdd1/&lt;/path&gt;
            &lt;/hdd1&gt;
        &lt;/disks&gt;
        &lt;policies&gt;
            &lt;moving_from_ssd_to_hdd&gt;
                &lt;volumes&gt;
                    &lt;hot&gt;
                        &lt;disk&gt;fast_ssd&lt;/disk&gt;
                        &lt;max_data_part_size_bytes&gt;1073741824&lt;/max_data_part_size_bytes&gt;
                    &lt;/hot&gt;
                    &lt;cold&gt;
                        &lt;disk&gt;hdd1&lt;/disk&gt;
                    &lt;/cold&gt;
                &lt;/volumes&gt;
                &lt;move_factor&gt;0.2&lt;/move_factor&gt;
            &lt;/moving_from_ssd_to_hdd&gt;
        &lt;/policies&gt;
    &lt;/storage_configuration&gt;
&lt;/yandex&gt;

</code></pre>
<h2 id="4-建表">4. 建表</h2>
<p>举个例子(实际使用必须确定清楚<strong>表引擎</strong>和<strong>聚合函数</strong>)，这里只是现场想个例子展示预聚合用法:</p>
<ul>
<li>表引擎使用: <code>MergeTree</code>，简单聚合函数</li>
<li>order by就是主键了</li>
<li>按时间分区</li>
<li>存储策略是优先SSD，上面那个</li>
<li>存储时间两个月</li>
</ul>
<pre><code class="language-sql">create table test.user_info
(
    record_time DateTime('Asia/Shanghai'),
    user String,
    cost Float64
)ENGINE = MergeTree()
ORDER BY (record_time,user)
PARTITION BY record_time
SETTINGS storage_policy = 'moving_from_ssd_to_hdd'
TTL record_time + INTERVAL 2 MONTH

-- 创建物化视图，比如每天每个用户cost总量
CREATE MATERIALIZED VIEW user_info_daily_mv
ENGINE = SummingMergeTree()
PARTITION BY toYYYYMM(day)
SETTINGS storage_policy = 'moving_from_ssd_to_hdd'
ORDER BY (day,user)
AS SELECT
    toStartOfDay(record_time) AS day,
    user ,
    sum(cost) as cost_count
FROM test.user_info
GROUP BY (day,user)
</code></pre>
<h1 id="集群版带副本">集群版（带副本）</h1>
<h2 id="1-zookeeper集群部署">1. zookeeper集群部署</h2>
<blockquote>
<p>正式环境貌似要设置zookeeper的日志清理</p>
</blockquote>
<h3 id="测试用compose直接起">测试用compose直接起</h3>
<pre><code>version: '3.1'

services:
  zoo1:
    image: zookeeper
    restart: always
    hostname: zoo1
    ports:
      - 2181:2181
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=0.0.0.0:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181

  zoo2:
    image: zookeeper
    restart: always
    hostname: zoo2
    ports:
      - 2182:2181
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=0.0.0.0:2888:3888;2181 server.3=zoo3:2888:3888;2181

  zoo3:
    image: zookeeper
    restart: always
    hostname: zoo3
    ports:
      - 2183:2181
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=0.0.0.0:2888:3888;2181
</code></pre>
<h3 id="手动安装">手动安装</h3>
<p>这部分删了，手动装挺麻烦的，这部分不大确定，虽然是能跑起来</p>
<h2 id="2-安装clickhouse">2. 安装clickhouse</h2>
<p>同上</p>
<h2 id="3-更新配置2分片2副本">3. 更新配置(2分片2副本)</h2>
<blockquote>
<p>不建议一个clickhouse-server拥有两个replicas来自不同shard。（除了性能问题，还会存在分布式DDL无法使用）<br>
见<a href="https://altinity.com/blog/2018/5/10/circular-replication-cluster-topology-in-clickhouse">CIRCULAR REPLICATION CLUSTER TOPOLOGY IN CLICKHOUSE</a>中的设计和<a href="https://github.com/ClickHouse/ClickHouse/issues/7611">github上issue所提的问题</a></p>
</blockquote>
<p>即每台机器是代表某个分片的某个副本</p>
<h3 id="所有配置文件">所有配置文件</h3>
<ul>
<li>config.xml (主配置文件)</li>
<li>users.xml (用户及配置限制)</li>
<li>storeage.xml (存储与策略配置)</li>
<li>metrika.xml（集群配置）</li>
</ul>
<h3 id="configxml-主配置文件">config.xml (主配置文件)</h3>
<pre><code>&lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt;
</code></pre>
<h3 id="storagexml-存储与策略配置">storage.xml (存储与策略配置)</h3>
<p>见单机版，冷热数据划分，实际规则</p>
<ul>
<li>part超过1G的迁移</li>
<li>ssd快不够容量时迁移</li>
</ul>
<h3 id="userxml用户配置">user.xml(用户配置）</h3>
<ul>
<li>配置文件见<a href="https://clickhouse.tech/docs/en/operations/settings/settings-users/">user settings</a></li>
<li>可以设置个默认密码，用户账号管理用<a href="https://clickhouse.tech/docs/en/operations/access-rights/#access-control">SQL-driven workflow</a></li>
</ul>
<h3 id="metrikaxml文件cluster配置">metrika.xml文件（cluster配置）</h3>
<ul>
<li>申请4台机器，两个分片，2个副本</li>
<li>1和2的机器在同一个分片下，3和4在同一个分片下</li>
</ul>
<pre><code class="language-xml">&lt;yandex&gt;
    &lt;remote_servers&gt;
        &lt;test_cluster&gt;
            &lt;shard&gt;
                &lt;!-- &lt;secret&gt;&lt;/secret&gt; --&gt;
                &lt;!-- Optional. Shard weight when writing data. Default: 1. --&gt;
                &lt;weight&gt;1&lt;/weight&gt;
                &lt;!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). --&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;!-- Optional. Priority of the replica for load balancing (see also load_balancing setting). Default: 1 (less value has more priority). --&gt;
                    &lt;!-- &lt;priority&gt;1&lt;/priority&gt; --&gt;
                    &lt;host&gt;xx.xxx.xx.xx1&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;xx.xxx.xx.xx2&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
            &lt;shard&gt;
                &lt;weight&gt;1&lt;/weight&gt;
                &lt;internal_replication&gt;true&lt;/internal_replication&gt;
                &lt;replica&gt;
                    &lt;host&gt;xx.xxx.xx.xx3&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                &lt;/replica&gt;
                &lt;replica&gt;
                    &lt;host&gt;xx.xxx.xx.xx4&lt;/host&gt;
                    &lt;port&gt;9000&lt;/port&gt;
                &lt;/replica&gt;
            &lt;/shard&gt;
        &lt;/test_cluster&gt;
    &lt;/remote_servers&gt;

    &lt;!-- 这里其实就是标记当前机器属于哪个shard，以及副本代号 --&gt;
    &lt;macros&gt;
        &lt;shard&gt;01&lt;/shard&gt;
        &lt;replica&gt;xx.xxx.xx.xx1&lt;replica&gt;
    &lt;/macros&gt;

    &lt;!-- ZK--&gt;
    &lt;zookeeper&gt;
        &lt;node index=&quot;1&quot;&gt;
            &lt;host&gt;xx.xxx.xx.xx1&lt;/host&gt;
            &lt;port&gt;2181&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index=&quot;2&quot;&gt;
            &lt;host&gt;xx.xxx.xx.xx2&lt;/host&gt;
            &lt;port&gt;2182&lt;/port&gt;
        &lt;/node&gt;
        &lt;node index=&quot;3&quot;&gt;
            &lt;host&gt;xx.xxx.xx.xx3&lt;/host&gt;
            &lt;port&gt;2183&lt;/port&gt;
        &lt;/node&gt;
    &lt;/zookeeper&gt;
    
&lt;/yandex&gt;
</code></pre>
<h3 id="宏定义统一建表语句">宏定义(统一建表语句)</h3>
<p>这里不设置layer层了，没必要</p>
<h4 id="记得每个节点都必须更新这个"><strong>记得每个节点都必须更新这个！！！</strong></h4>
<h4 id="注意同一个shard的节点的shard配置必须一致"><strong>注意同一个shard的节点的shard配置必须一致！！</strong></h4>
<pre><code class="language-xml">&lt;macros&gt;
    &lt;shard&gt;01&lt;/shard&gt;
    &lt;replica&gt;hostname&lt;/replica&gt;
&lt;/macros&gt;
</code></pre>
<h3 id="最终配置文件">最终配置文件</h3>
<h4 id="丢到configd目录下覆盖默认配置">丢到config.d目录下覆盖默认配置</h4>
<ul>
<li>xxx.xml（合并了listen配置和storeage配置）</li>
<li>metrika.xml（单独，这部分是热更新，无需重启）</li>
</ul>
<h4 id="用户配置丢到userd目录下覆盖默认配置">用户配置丢到user.d目录下覆盖默认配置</h4>
<ul>
<li>users.xml</li>
</ul>
<h3 id="集群样例">集群样例</h3>
<p>具体可以去查看system的cluster表，看看集群配置是否生效了</p>
<h2 id="4-每个节点启动click-server确定互通">4. 每个节点启动click server(确定互通)</h2>
<ul>
<li>9000端口用于clickhouse互通与clickhouse-client</li>
<li>8123端口为http访问</li>
</ul>
<h2 id="5-建表">5. 建表</h2>
<pre><code class="language-sql">/*分布式DDL建库*/
CREATE DATABASE test ON CLUSTER 'test_cluster'

/*分布式DDL建库，树引擎其实可以不写，就是默认值来的*/
CREATE TABLE test.test_info_local ON CLUSTER 'test_cluster'(
    ......
)ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/test/test_info_local','{replica}')
PARTITION BY record_time
ORDER BY (record_time)
TTL record_time + INTERVAL 6 MONTH
SETTINGS storage_policy = 'moving_from_ssd_to_hdd'

/*分布式DDL建立原始数据分布表*/
CREATE TABLE  test.test_info_local on CLUSTER 'test_cluster'
as test.test_info
ENGINE = Distributed(test_cluster,test,test_info_local,rand())

/*以下不写了，基本看官方文档详细点*/
/*物化视图有实际table的，只不过默认隐藏，我们也可以手动创建*/


/*分布式DDL创建物化视图*/
/*这里因为提前建好复制表了，不需要再设置engine那些*/



/**分布式DDL 创建全局视图*/

</code></pre>
<ul>
<li>分布式DDL跑一次就够了</li>
</ul>
<h2 id="6-实际应用">6. 实际应用</h2>
<h3 id="写本地表">写本地表</h3>
<p>最好直接写本地表，因为直接写分布式表，在clickhouse上还要再同步一次，实际上相当于双写了</p>
<h3 id="读分布式表">读分布式表</h3>
<h2 id="7-测试">7. 测试</h2>
<p>性能方面简单的可以看<a href="https://clickhouse.tech/docs/en/introduction/performance/">官方说明</a>，尽量批量写，这里其实还会涉及到clickhouse里part的概念，不细说了</p>
<blockquote>
<p>We recommend inserting data in packets of at least 1000 rows, or no more than a single request per second.</p>
</blockquote>
<h2 id="8-个人经验">8. 个人经验</h2>
<ul>
<li>表引擎要留意清楚，个人一般使用最多的是<code>SummingMergeTree</code>或者<code>AggregatingMergeTree</code>，分布式就加个<code>replica</code>前缀</li>
<li>clickhouse允许相同主键的数据存在，是否去重以及去重的规则具体看表引擎决定。</li>
<li>clickhouse不保证part merge的时机，这个是坑，某些实际应用场景时就会感受到的</li>
<li>可以对视图再进行视图的聚合，不过要注意，本质上是insert事件触发了视图，视图并不知道原表的数据</li>
<li>分布式用到zookeeper（这玩意还是没那么好用），这里依赖它主要是在分布式DDL执行和ReplicaMergeTree主备状态同步上（对于每个insert的数据part，clickhouse都会通过几个事务将记录添加到zookeeper上）</li>
</ul>
<p>有问题欢迎留言~</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gridea扩展(一)- 代码高亮]]></title>
        <id>https://geraltxli.github.io/post/gridea-kuo-zhan/</id>
        <link href="https://geraltxli.github.io/post/gridea-kuo-zhan/">
        </link>
        <updated>2021-05-30T14:27:40.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<p>没想到Gridea默认没有代码高亮，那只能自己来了，使用<strong>highlightjs</strong>，修改一下主题里代码，   引入一下就好了。</p>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<p>没想到Gridea默认没有代码高亮，那只能自己来了，使用<strong>highlightjs</strong>，修改一下主题里代码，   引入一下就好了。</p>
</blockquote>
<!-- more -->
<h2 id="代码高亮">代码高亮</h2>
<ol>
<li>下载<a href="https://highlightjs.org/">highlight.js</a>，点<code>Get Version</code>去下载进行，里面也不少各种语言等之类的选择，一般默认就够用了</li>
</ol>
<figure data-type="image" tabindex="1"><img src="https://geraltxli.github.io/post-images/1622428479016.png" alt="" loading="lazy"></figure>
<ol start="2">
<li>
<p>下载完后解压，简单点就把整个文件夹<code>highlight</code>都丢到gridea数据目录的<code>static</code>目录下（不过其实也可以直接copy里面的<code>highlight.pack.js</code>和<code>styles</code>目录）</p>
</li>
<li>
<p>去<code>themes</code>目录下修改，一般只有文章会用到，所以直接修改<code>post.ejs</code>。在head里加几行代码就行了，这里我选了<code>github-dark</code>样式，这个看自己爱好</p>
</li>
</ol>
<pre><code class="language-html">&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;&lt;%= themeConfig.domain %&gt;/highlight/styles/github-dark.css&quot;&gt;
    &lt;script src=&quot;&lt;%= themeConfig.domain %&gt;/highlight/highlight.pack.js&quot;&gt;&lt;/script&gt;
    &lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golang-AES加密（CBC模式，PKCS7填充）]]></title>
        <id>https://geraltxli.github.io/post/golang-aes-jia-mi-cbc-mo-shi-pkcs7-tian-chong/</id>
        <link href="https://geraltxli.github.io/post/golang-aes-jia-mi-cbc-mo-shi-pkcs7-tian-chong/">
        </link>
        <updated>2021-05-30T13:33:56.000Z</updated>
        <summary type="html"><![CDATA[<p><a href="https://github.com/FakerGit/go-tools/tree/master/encrypt">代码地址</a></p>
<blockquote>
<p>对称加密算法，即加密和解密使用一样的密钥的加解密算法。<br>
分组密码（block cipher），是每次只能处理特定长度的一块（block）数据的一类加解密算法。<br>
目前常见的对称加密算法DES、3DES、AES都是属于分组密码。</p>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<p><a href="https://github.com/FakerGit/go-tools/tree/master/encrypt">代码地址</a></p>
<blockquote>
<p>对称加密算法，即加密和解密使用一样的密钥的加解密算法。<br>
分组密码（block cipher），是每次只能处理特定长度的一块（block）数据的一类加解密算法。<br>
目前常见的对称加密算法DES、3DES、AES都是属于分组密码。</p>
</blockquote>
<!-- more -->
<h3 id="背景">背景</h3>
<p>Golang没有像PHP那样提供一个现成的aes加密函数，不过标准库里有crypto，利用里面的aes等可以自己封装个加密函数，不过需要理解下整个加解密的过程和原理</p>
<!-- more -->
<hr>
<h3 id="aes加密详解">AES加密详解</h3>
<h4 id="1-参考文章golang-中aes加密详解">1. 参考文章<a href="https://blog.csdn.net/xiaohu50/article/details/51682849">golang 中AES加密详解</a></h4>
<h4 id="2-这里使用的是aes加密中的cbc模式块加密需要划分成整数长度相等个消息块不断加密串行分组长度是固定128位但密钥的长度可以使用128位192位或者256位这里指的是bit即密钥162432长度对应aes-128-aes-192-aes-256">2. 这里使用的是AES加密中的CBC模式，块加密需要划分成整数长度相等个消息块不断加密（串行），分组长度是固定128位，但密钥的长度可以使用128位，192位或者256位（这里指的是bit），即密钥16，24，32长度对应AES-128, AES-192, AES-256。</h4>
<h4 id="3初始向量要求随机但不需要保密">3.初始向量要求随机，但不需要保密。</h4>
<hr>
<h3 id="代码">代码</h3>
<p>自己研究代码比较清晰，根据<a href="https://golang.org/src/crypto/cipher/example_test.go">golang标准库AES实例代码</a>，再参考网上的PKCS7填充，最后进行base64的编码（因为加密后有些字符不可见）。最后Encrypt和Dncrypt两个就是AES加解密（CBC模式，PKCS7填充）封装后的函数，密钥位数限定16,24,32（要注意的是密钥无论多少，blocksize都是固定16）</p>
<pre><code class="language-go">package encrypt

import (
   &quot;bytes&quot;
   &quot;crypto/aes&quot;
   &quot;io&quot;
   &quot;crypto/rand&quot;
   &quot;crypto/cipher&quot;
   &quot;encoding/base64&quot;
)

/*CBC加密 按照golang标准库的例子代码
不过里面没有填充的部分,所以补上
*/

//使用PKCS7进行填充，IOS也是7
func PKCS7Padding(ciphertext []byte, blockSize int) []byte {
   padding := blockSize - len(ciphertext) % blockSize
   padtext := bytes.Repeat([]byte{byte(padding)}, padding)
   return append(ciphertext, padtext...)
}

func PKCS7UnPadding(origData []byte) []byte {
   length := len(origData)
   unpadding := int(origData[length-1])
   return origData[:(length - unpadding)]
}

//aes加密，填充秘钥key的16位，24,32分别对应AES-128, AES-192, or AES-256.
func AesCBCEncrypt(rawData,key []byte) ([]byte, error) {
   block, err := aes.NewCipher(key)
   if err != nil {
   	panic(err)
   }

   //填充原文
   blockSize := block.BlockSize()
   rawData = PKCS7Padding(rawData, blockSize)
   //初始向量IV必须是唯一，但不需要保密
   cipherText := make([]byte,blockSize+len(rawData))
   //block大小 16
   iv := cipherText[:blockSize]
   if _, err := io.ReadFull(rand.Reader,iv); err != nil {
   	panic(err)
   }

   //block大小和初始向量大小一定要一致
   mode := cipher.NewCBCEncrypter(block,iv)
   mode.CryptBlocks(cipherText[blockSize:],rawData)

   return cipherText, nil
}

func AesCBCDncrypt(encryptData, key []byte) ([]byte,error) {
   block, err := aes.NewCipher(key)
   if err != nil {
   	panic(err)
   }

   blockSize := block.BlockSize()

   if len(encryptData) &lt; blockSize {
   	panic(&quot;ciphertext too short&quot;)
   }
   iv := encryptData[:blockSize]
   encryptData = encryptData[blockSize:]

   // CBC mode always works in whole blocks.
   if len(encryptData)%blockSize != 0 {
   	panic(&quot;ciphertext is not a multiple of the block size&quot;)
   }

   mode := cipher.NewCBCDecrypter(block, iv)

   // CryptBlocks can work in-place if the two arguments are the same.
   mode.CryptBlocks(encryptData, encryptData)
   //解填充
   encryptData = PKCS7UnPadding(encryptData)
   return encryptData,nil
}


func Encrypt(rawData,key []byte) (string,error) {
   data, err:= AesCBCEncrypt(rawData,key)
   if err != nil {
   	return &quot;&quot;,err
   }
   return base64.StdEncoding.EncodeToString(data),nil
}

func Dncrypt(rawData string,key []byte) (string,error) {
   data,err := base64.StdEncoding.DecodeString(rawData)
   if err != nil {
   	return &quot;&quot;,err
   }
   dnData,err := AesCBCDncrypt(data,key)
   if err != nil {
   	return &quot;&quot;,err
   }
   return string(dnData),nil
}
</code></pre>
<pre><code class="language-json">{
    &quot;json&quot;:&quot;123&quot;,
    &quot;hah&quot;:&quot;123&quot;
}
</code></pre>
<hr>
<p><a href="https://github.com/FakerGit/go-tools">github代码地址</a></p>
<hr>
<p>参考：</p>
<ol>
<li><a href="https://blog.csdn.net/xiaohu50/article/details/51682849">golang 中AES加密详解</a></li>
</ol>
]]></content>
    </entry>
</feed>